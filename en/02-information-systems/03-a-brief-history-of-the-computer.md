## A Brief History of the Computer as an Information System

The computer was not always digital. In fact, the word "computer" originally referred to, simply, a person that calculates.[^computer1] But mental calculations do not come easily to the human mind,[^turing1] and from a very early time people sought tools to help them count and compute.[^io9] The abacus is one example that is commonly known, which was used by the Sumerians and Egyptians to count and tally numbers since before 2000 B.C.E.[^ifrah] Millenia later, in the sixteenth century, Galileo Gallilei and others developed the geometrical sector, or compass, which was two conjoined rulers inscribed with scales that helped solve various mathematical problems. A descendant of the sector was the slide rule, developed around 1642, that found use as recently as the 1960s by NASA engineers, pilots, and astronauts working on the Apollo moon missions.[^si] Charles Babbage created the difference engine in the 1850s, that used wheels and gears to compute polynomial functions.[^CITE] A very curious mechanical calculator called the water integrator, which took up space the size of a room, was developed around 1928 in the Soviet Union.[^trogemann] Water residing in compartments represented stored numbers involved in calculations, and the water was manipulated such that its rate of flow through various piping represented certain mathematical operations. The water integrator reportedly could model and help solve complex mathematical principles such as calculus and differential equations.

[^computer1]: "Once upon a time, a 'computer' was a human being, usually female, who did calculations set for her by men in suits." John Naughton, The true fathers of computing, The Guardian, http://www.theguardian.com/technology/2012/feb/26/first-computers-john-von-neumann (Feb. 25, 2012).

[^turing1]: See Alan Turing, "On computable numbers, with an application to the Entscheidungsproblem", Proceedings of the London Mathematical Society, Series 2, 42 (1936-7), pp 230–265, at 231, available at http://plms.oxfordjournals.org/content/s2-42/1/230.full.pdf (comparing "a man in the process of computing a real number to a machine which is only capable of a finite number of conditions").

[^io9]: An interesting gallery of mechanical calculators and computers can be found on the blog posting located at http://io9.com/the-strangest-and-most-beautiful-calculators-humanity-h-1668731927 (accessed Dec. 14, 2014).

[^ifrah]: Ifrah, Georges, The Universal History of Computing (2001).

[^si]: See "Slide Rule, 5-inch, Pickett N600-ES, Apollo 13," Smithsonian National Air and Space Museum, http://airandspace.si.edu/collections/artifact.cfm?object=nasm_A19840160000 (accessed Dec. 14, 2014). During the Apollo missions, astronaut crews carried a slide rule for routine calculations that were not performed by the on-board critical guidance and navigation computer. Id. See generally, The Slide Rule: A Computing Device That Put A Man On The Moon, NPR, http://www.npr.org/blogs/ed/2014/10/22/356937347/the-slide-rule-a-computing-device-that-put-a-man-on-the-moon (Oct. 22, 2014).

[^trogemann]: Georg Trogemann, Alexander Y. Nitussov, Wolfgang Ernst, Computing in Russia (2001).

These and other mechanical devices or automata, although incredibly useful, were limited to solving calculations for a particular problem or a narrow set of problems. They were not general-purpose in the sense that they could not be readily modified by users, changing their operation through a different sequence of instructions, in order solve a different kind of calculation. Charles Babbage proposed such a general-purpose mechanical system that would be programmed by punch cards, called the Analytical Engine, but was not able to complete it.[^CITE][^lovelace].

[^lovelace]: It was noted that at the time that Babbage believed "he can, by his engine, form the product of two numbers, each containing twenty figures, in three minutes." See "Sketch of the Analytical Engine Invented by Charles Babbage," Charles Babbage on the Principles and Development of the Calculator at 242 (1989). Some calculations would have required supplying punch cards numbering in the tens of thousands. Id. Such calculations are handled internally by modern computer processors in less than a microsecond.[CITE]

The general-purpose computer did not arrive until 1936, when British mathematician Alan Turing published the mathematical foundation for the *universal computing machine*.[^turing2] Turing described a hypothetical machine that would read and print a sequence of symbols on a finite paper tape according to a set of rules. Turing proposed the idea of "a single [such] machine that can be used to compute any computable sequence." Stated more generally, Turing's thesis was that his universal computing machine could mimic any other computing machine if supplied with a description of its operation. 

[^turing2]: Alan Turing, "On computable numbers, with an application to the Entscheidungsproblem", Proceedings of the London Mathematical Society, Series 2, 42 (1936-7), pp 230–265. Turing's publication competed with a parallel development by Alfonso Church. Together, their unified thesis is often referred to as the Church-Turing principle.

The universal nature of the machine Turing described, while a seemingly simple concept, has profound implications. If a machine could be reduced to a description in a series of symbols, it is no longer a *specific* machine. It is a *generic* machine because it can mimic the performance of other computing machines, or its description can be be applied to another computer to be imitated there. [^weisenbaum62] The point, here, is that the user need only think up the bare idea, and it is mathematical truth that one can write down a procedure in way the computer can understand in order to make the computer perform that idea.[^weisenbaum64] In short, if you could conceive it, you could build a computer that does it.[^deutch]

[^weisenbaum62]: Joseph Weisenbaum, Computer Power and Human Reason, 62 (1976). 

[^weisenbaum64]: See Joseph Weisenbaum, Computer Power and Human Reason, 64 (1976) ("Therefore, whenever we believe we understand a phenomenon in terms of knowing its behavioral rules, we ought to be able to express our understanding in the form of a computer program.").

[^deutch]: The Church–Turing–Deutsch principle states that a universal computing device can simulate every natural *physical* process, not just abstract mathematical ideas.  See Deutsch, D. (1985). "Quantum theory, the Church–Turing principle and the universal quantum computer". Proceedings of the Royal Society (London) (400): 97–117.

Essentially all mainstream computer environments are universal in this sense. Every modern computer can mimic every other computer.[^weisenbaum63] A tangible demonstration of this concept can be seen when desktop computers "emulate" other computers, such as when a desktop computer emulates an Atari 2600 console to run decades-old video games,[^atari] a computer created by Apple runs software designed to operate only on Microsoft Windows computers,[^emulators] or when an Apple computer emulates a complete Microsoft Windows operating system environment.[^emulators]

[^weisenbaum63]: Joseph Weisenbaum, Computer Power and Human Reason, 63 (1976).

[^atari]: Stella: A multi-platform Atari 2600 VCS emulator, http://stella.sourceforge.net/docs/index.html.

[^emulators]: A Comparison of Solutions for Running Windows or Linux Software on a Macintosh, http://www.macwindows.com/emulator.html.

This universality is what makes computers unique from all other tools that have preceeded their arrival, all of which are machines that solve only a very specific problem. In principle, nothing separates an individual from writing down an idea in a series of steps--an algorithm--in language he or she understands--like English--and translating that procedure into a program in a language that can be understood and run by a modern computer. One of the early innovators of computing systems, John von Neumann described, what happens after this translation takes place:

> An automatic computing system is a (usually highly composite) device, which can carry out instructions to perform calculations of a considerable order of complexity. The instructions which govern this operation must be given to the device in absolutely exhaustive detail....These instructions must be given in some form which the device can sense: Punched into a system of punchcards or on teletype tape, magnetically impressed on steel tape or wire, photographically impressed on motion picture film, wired into one or more fixed or exchangeable plugboards—this list being by no means necessarily complete. All these procedures require the use of some code to express the logical and the algebraical definition of the problem under consideration, as well as the necessary numerical material. Once these instructions are given to the device, it must be able to carry them out completely and without any need for further intelligent human intervention. At the end of the required operations the device must record the results again in one of the forms referred to above. The results are numerical data; they are a specified part of the numerical material produced by the device in the process of carrying out the instructions referred to above.[^vonneumann1]

[^vonneumann1]: John von Neumann, First Draft of a Report on the EDVAC (June 30, 1945)

There are limits to this principle however, in that Turing's universal computer is a theoretical ideal. Actual computers, however, are physical devices bound by limitations forced upon it by the physical world. Computers take time to compute and have limited room to store data and information. Thus, while a given computer could, in theory, perform any procedure intended to solve a problem, in practice it may not be able to effectively do so because its operation may not complete for millions of years.[^factoring] Since the introduction of the universal computer, much innovation has been focused on improving the architecture of the general purpose computer to make it faster or more efficient.

[^factoring]: For example, much of modern cryptography and encrypted communications rely on "one-way functions." These are mathematical formulas that are quickly calculated to give a solution, but to undo the calculation (if one first had the solution in hand) requires relatively much longer time. Security relies solely upon the fact that the reversal of these formulas by a computer are *impractical* in time, even if possible. One can rest assured that a message will remain secret long after its secrecy is no longer important

The first big step taken to advance a tangible architecture for the universal computer was taken by John von Neumann around 1945, when he developed the concept of a "stored-program computer." von Neumann was the leader of the computer project at Princeton Institute of Advanced Study, charged with development of the EDVAC computer. Previous iterations, such as the ENIAC project, were not universal computing machines rather something built out of wartime necessity to specifically calculate artillery trajections and operated by re-routing cables and flipping switches.  Instead of supplying paper tape as in Turing's hypothetical or movable cables and switches as in ENIAC to read instructions, von Neumann designed a machine that stored the instructions that the computer would follow in its memory alongside its working data. As one commentator described:

> Thanks to Turing's abstract work, , von Neumann knew that, by making use of coded instructions stored in memory, a single machine of fixed structure could in principle carry out any task for which an instruction table can be written.... von Neumann saw that this was the means to make concrete the abstract universal computing machine of "On Computable Numbers."[^copeland1]

[^copeland1]: B. Jack Copeland and Diane Proudfoot, "The Computer, Artificial Intelligence, and the Turing Test," Alan Turing: Life and Legacy of a Great Thinker,  321 (Ed. Christof Teuscher 2004).

In such a machine, the instructions to follow are loaded into storage, and then run. Complex procedures to control the operation of the computer could now but implemented, such as loops and conditional jumps (if *this* then do *that*). This ability is what makes the computer a general-purpose tool of extraordinary power.[^reilly] By loading new procedures into the computer, what was once a universal machine capable of any conceivable computation, is now a specific machine that will accomplish, exactly, only a specific function designated by the whims of its programmer.

[^reilly]: Edwin D. Reilly, Milestones in Computer Science and Information Technology 245 (2003).

In his "First Draft of a Report on the EDVAC," von Neumann analogized the computer architecture he created, seen in Figure ____, to a human.[^vonneumann] The computer required a central component comprising an arithmetic unit and control unit, as well as a memory unit. He described these as corresponding to the associative neurons in the human nervous system, as in the brain. All calculation and operation would be carried out by these components. But von Neumann also recognized "a necessity of getting the original definitory information from outside into the device, and also getting the final information, the results, from the device to the outside."[^vonneumann] These functions would be performed by typing, punching, photographing light, magnetizing tape or an analogous means that could be operated or sensed by humans.

[^vonneumann]: John von Neumann, First Draft of a Report on the EDVAC (June 30, 1945).

![vonneumann-architecture](http://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/Von_Neumann_Architecture.svg/1000px-Von_Neumann_Architecture.svg.png) 

What is striking about von Neumann's architecture and his description of it is the resemblance to our earlier description of an information processor. The computer receives data from the environment as input, processes that data into something useful, and outputs the processed information. At a high-level, we can think of the computer as a drop-in replacement for a human node in an information processing system. As MIT professor Joseph Weisenbaum explained:

> We have already agreed that it is entirely proper and even useful ... to see man as an information processor. And since the computer, the Turing machine, is a universal information processor, it is natural to compare man as seen from that perspective with the computer.[^weisenbaum170]

[^weisenbaum170]: Joseph Weisenbaum, Computer Power and Human Reason, 170 (1976).

Of course, computers and humans differ in the internal architecture of their tangible embodiments. One consists of electrical conductors typically made today out of copper and silicon, and the other is fashioned from a network of biological, organic neurons. The differences in the internal architectures means they have different characteristics. Digital computers rely on hard, exacting calculations, which is a result of the binary on-and-off nature of their underlying logic. Humans, on the other hand, think about the world in a fuzzy, probabilistic manner.[^CITE]

And just as the wiring of each human brain is unique, so to do differences exist in general-purpose digital computers. Although the von Neumann architecture is the most common foundation for modern general-purpose computers (and especially in commercial desktop and handheld computers) it is not the only such architecture. The Harvard architecture, for example, is a different kind of stored-program computer that holds program instructions and working data in separate memories, rather than a single memory.[^CITE] And within the von Neumann architecture, memory can be deconstructed into different kinds of physical embodiments, such as a combination of external RAM, comprising fast semiconductor chips, and disk drives, which are slower magnetic discs read from and written to by a moveable arm.[^ram]

[^ram]: In modern computer architecture, there are significantly many more permutations of memory structures involved that are employed based upon their various trade-offs of speed, energy, and reliability, including: internal computer registers, central cache, external ROM, external RAM, solid-state drives, magnetic discs, optical discs, magnetic tape, and even separate general-purpose computers programmed to store information received over a communications network (commonly referred to as "cloud" storage, provided by services Apple iCloud, Google Drive, or Dropbox).

But these distinctions play no role in evaluating the digital computer as an information processor. A computer can be provided measurements for the energy, angle, wind and gravitational influences on ballistic artillery, and its program can calculate a trajectory to hit its target. But so can a human with pen and paper and an understanding of the mathematics of physics. A human can also, without aid of any digital devices or writing to store intermediate results, can hear the crack of a bat, observe the flight of a baseball, and with an eager run and outstretched hand catch the pop fly as it reaches the end of its arc. When we are discussing the processing of data into information, the means by which this transformation is carried out does not matter in the first instance. When information is the subject, there is typically nothing innovative about the type of information processing device employed, whether that tool is a digital computer, a mechanical device, or the human mind itself. Nor would it matter if a new breed of information processor were developed in the form of a better computer--or even a replacement to the digital computer as we know it. The end result is the same: a tool is used to turn data is turned into useful information to create knowledge.
